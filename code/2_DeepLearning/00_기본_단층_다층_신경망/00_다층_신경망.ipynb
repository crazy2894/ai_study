{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다층 신경망 (Multi Layer Preceptron)\n",
    "\n",
    "- 인공지능 > 머신러닝 < 딥러닝\n",
    "    - 딥러닝 : 신경망, 비 선형 문제에서 좀 더 장점을 보인다.\n",
    "        - **활성화 함수 : 비선형 문제의 핵심**\n",
    "    - 전통적인 머신러닝\n",
    "        - 선형적인 분포와 가정을 기반한다.\n",
    "        - 특징 추출 과정에서 도메인 지식을 요한다.\n",
    "        - 선택한 피쳐의 퀄리티에 크게 의존\n",
    "\n",
    "## 기본적으로 알아야 할 이론\n",
    "\n",
    "- 신경망 학습 프로세스\n",
    "- 활성화 함수, 비용(Error) 함수\n",
    "- 오차 역전파 (back propagation)\n",
    "- 옵티마이저\n",
    "- 오버피팅\n",
    "\n",
    "## 종류\n",
    "\n",
    "- FNN : Feedforward neural Net - 지도학습\n",
    "- CNN : Convolution neural Net - 지도학습\n",
    "    - 사전 학습 모델\n",
    "    - 전이 학습\n",
    "    - 오브젝트 Detection 모델 : YOLO, SSD, R-CNN 종류 등\n",
    "- RNN : Recurrent neural Net - 지도학습\n",
    "    - Seq2Seq\n",
    "    - LSTM, GRU\n",
    "- LM : Language Model\n",
    "    - Transformer(Attention is all you need)\n",
    "    - BERT & BERT 파생들\n",
    "    - GPT 모\n",
    "- AE : Auto Encoder - Self Supervised model\n",
    "\n",
    "# 딥러닝의 구조\n",
    "\n",
    "## 노드 (Node, 뉴런)\n",
    "\n",
    "정보를 받아 입 출력하는 기본단위.\n",
    "\n",
    "여러 입력을 받아 하나의 출력을 생성\n",
    "\n",
    "- 역할 :\n",
    "    1. 신호 처리 : 곱과 합을 이용\n",
    "    2. 비 선형성 : 활성화 함수 이용\n",
    "    3. 정보 통합 : 곱하여 더하므로 정보를 통합\n",
    "\n",
    "## 가중치 (weights)\n",
    "\n",
    "각 연결선에 할당.\n",
    "\n",
    "영향을 끼치는 정도\n",
    "\n",
    "- 역할 :\n",
    "    - 입력 중요도 : 각 피쳐 별 중요도 부여\n",
    "    - 학습 : 가중치를 업데이트 하며 학습한다.\n",
    "    - 신호 전달\n",
    "\n",
    "## 편향 노드 (bias node)\n",
    "\n",
    "선형 회귀에서의 절편 : 즉 평행 이동, 모델 일반화에 큰 도움을 준다\n",
    "\n",
    "활성화 함수가 결정하는 임계 값 이상일 때 노드를 활성화 한다.\n",
    "\n",
    "(바이어스 값이 노드의 활성화를 조절한다.)\n",
    "\n",
    "- 활성화 임계를 조정\n",
    "  편향을 통해 모델은 입력 데이터가 없거나 0에 가까울 때도 활성화 <br>\n",
    "-> 모델의 학습 능력과 일반화 능력 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 레이어\n",
    "\n",
    "- 입력 레이어 및 출력 레이어 : 각 1층 씩\n",
    "- 은닉층은 설계자의 결정\n",
    "- 레이어 별 이름\n",
    "    - Single Layer Perceptron : 단층, 은닉 층 없음\n",
    "    - Multi Layer Perceptron : 다층, 은닉 층 있음\n",
    "    - Shallow Neural Net : 얕은 신경망, 은닉 층은 하나\n",
    "    - Deep Neural Net : 심층 신경망, 은닉 층은 두개 이상\n",
    "\n",
    "### 단층 퍼셉트론 및 다층 퍼셉트론\n",
    "\n",
    "- 단층 퍼셉트론의 한계점 : XOR 같은 비선형 문제를 해결 불가능 하다. 실제 세상은 대부분 비선형\n",
    "- 단층 퍼셉트론 에서의 역 전파? : 역 전파의 개념이 생긴 시점은 다층 퍼셉트론 부터 이다.\n",
    "- 다층 퍼셉트론의 특징 : 비선형성은 활성화 함수로 표현 된다(**범용 근사자 : 충분히 크고 복잡한 문제라도 이론적으로 학습이 가능하다.**), 역 전파로 오류를 최소화 한다.\n",
    "\n",
    "## 인공 신경망의 프로세스\n",
    "\n",
    "- 프로그래밍 적 관점 :\n",
    "    1. 데이터 세팅\n",
    "    2. 독립 변수 및 종속변수 관계 파악 및 비용함수 최소화\n",
    "    3. 모델 성능 평가\n",
    "    4. 종속 변수의 예측\n",
    "- 내부 거동의 관점 :\n",
    "    1. Weights 및 biases 초기화\n",
    "    2. 순 전파\n",
    "    3. 오차 계산\n",
    "    4. 오류 함수의 역 전파 \n",
    "    5. 가중치 업데이트\n",
    "    6. 과 적합 및 정규화 : L1 및 L2 정규화, 드롭 아웃 등\n",
    "    7. 반복 학습\n",
    "\n",
    "## 선형 회귀 모형과 신경망 모델의 차이\n",
    "\n",
    "- 선형 회귀 모형 : $\\hat{y} = b_0 + b_1  X_1 +  b_2  X_2 + b_3  X_3 + b_4  X_4$\n",
    "- 위의 선형 회귀 모형과 신경망 모델의 차이 : 은닉 층의 존재 여부\n",
    "\n",
    "## 출력 노드의 수\n",
    "\n",
    "- 회귀 문제 : 1개\n",
    "- 분류 문제 : 정답 라벨의 개 수."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
