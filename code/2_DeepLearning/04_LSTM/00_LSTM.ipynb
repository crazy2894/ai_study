{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "RNN 에서의 단점인 오래된 단어 정보에 대하여 희미해 지는 문제를  완화 시킨다.\n",
    "\n",
    "GRU 보다 LSTM 더 우수하지만 복잡하다 (GRU : RNN의 간단한 버전)\n",
    "\n",
    "**Gate** 를 이용한다\n",
    "\n",
    "<img src=\"../../img/_08_LSTM/LSTM01.png\" width=\"700\">\n",
    "\n",
    "## 기억 셀 ($C_t$)\n",
    "\n",
    "- 오래된 단어를 보다 잘 기억\n",
    "- 게이트가 이 역할을 한다.\n",
    "- **기억 셀은 하나의 벡터!**\n",
    "- 차원 수 : hidden state의 원소 = $c_t$ 벡터의 원소 = LSTM 층의 은닉 노드의 수\n",
    "\n",
    "## 기억 셀의 업데이트\n",
    "\n",
    "- 이전 정보를 삭제(기억) 하거나 , 새로운 정보를 추가\n",
    "- 필요 없는 정보를 잊어 버리고 중요한 정보를 추가한다.\n",
    "- 종류 : forget, input, output 게이트가 있다.\n",
    "- 역할 : **$c_{t-1}$  을 새로** 업데이트 하여 **$c_t$ 계산**\n",
    "    - 이후 $c_t$, 단어 임베딩 벡터 $t$, $h_{t-1}$ 을 이용하여 $h_t$  를 계산하고 다음층으로 넘긴다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
