{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비용함수\n",
    "\n",
    "- 예측과 정답과의 차이를 본다.\n",
    "- 가중치를 조정하기 위해\n",
    "- 네트워크가 최적을 예측 수행 하도록 학습\n",
    "\n",
    "- 역할 :\n",
    "  모델 평가 : 예측과 실제의 차이를 수치적 표현하기 위해\n",
    "  모델 최적화 : 비용함수의 값을 최소화, 가중치 편향 조정 - 학습 과정\n",
    "\n",
    "## 평균 제곱 오차\n",
    "\n",
    "### 회귀 문제\n",
    "\n",
    "- 평균 제곱 오차\n",
    "    \n",
    "    $\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y - \\hat{y})$\n",
    "    \n",
    "    - 나중에 루트를 씌어 실제와 비교 (제곱이므로)\n",
    "    - 큰 오차에 더 큰 가중치 (제곱이므로)\n",
    "    - 일반적인 사용\n",
    "- 평균 절대 오차\n",
    "    \n",
    "    $\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y - \\hat{y}|$\n",
    "    \n",
    "    - 이상치에 덜 민감\n",
    "    - 값 자체로의 해석이 가능하다.\n",
    "- 허브 손실\n",
    "    \n",
    "    $L_\\delta(y,\\hat{y}) = \\begin{cases} \\frac{1}{2} (y - \\hat{y})^2 & \\text{if} \\quad|y - \\hat{y}| \\leq \\delta \\\\ \\delta |y - \\hat{y}| - \\frac{1}{2} \\delta^2 & \\text{if} \\quad|y-\\hat{y}| > \\delta \\end{cases}$\n",
    "    \n",
    "    - MAE 와 MSE 와의 조합\n",
    "    - 오차가 클 때 : MSE, 오차가 작을 때 : MAE\n",
    "    - 여기서 $\\delta$ 는 임계값\n",
    "- 로그 코사인 유도\n",
    "    \n",
    "    $\\log - \\cosh = \\frac{1}{N} \\sum^{N}_{i = 1} \\log({\\cosh (\\hat{y}-y)})$\n",
    "    \n",
    "    - 오차의 cosh 의 log\n",
    "    - 이상치에 매우 강하다, 최적화에 장점을 가지고 있다.\n",
    "\n",
    "### 분류 문제\n",
    "\n",
    "- 교차 엔트로피 손실\n",
    "    - 이진 분류에서 사용(다중 분류시 Categorical 교체 엔트로피 이용)\n",
    "    - 예측과의 차이를 보여줌\n",
    "    - 실제와 가까울 수록, 손실은 적어짐\n",
    "- 힌지 손실\n",
    "    - 주로 SVM 에서 사용\n",
    "    - 이진분류의 마진오류의 최소화\n",
    "- 제곱 힌지 손실\n",
    "    - 힌지 손실의 제곱\n",
    "    - 이상치에 민감\n",
    "- 로지스틱 손실\n",
    "    - 이진분류에 적합, 로지스틱 회귀에서 자주 사용\n",
    "    - 예측과 실제 사이의 로그 손실\n",
    "- 포칼 손실\n",
    "    - 클래스 불균형이 심한 문제\n",
    "    - 정답 보다, 오답 분류에 대한 가중치 부여\n",
    "\n",
    "## 비용함수 예제\n",
    "\n",
    "- 회귀 문제\n",
    "    \n",
    "    ```python\n",
    "    # 손실 함수 테스트\n",
    "    import numpy as np\n",
    "    \n",
    "    # 예측값과 실제값\n",
    "    y_true = np.array([1.0, 2.0, 3.0])\n",
    "    y_pred = np.array([1.1, 1.9, 3.1])\n",
    "    ```\n",
    "    \n",
    "    ```python\n",
    "    # 평균 제곱 오차 (Mean Squared Error)\n",
    "    def mean_squared_error(y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mse\n",
    "    ```\n",
    "    \n",
    "    ```python\n",
    "    # 평균 절대 오차 (Mean Absolute Error)\n",
    "    def mean_absolute_error(y_true, y_pred):\n",
    "        return np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mae\n",
    "    ```\n",
    "    \n",
    "    ```python\n",
    "    # 허브(Huber) 손실\n",
    "    def huber_loss(y_true, y_pred, delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = np.abs(error) <= delta\n",
    "        squared_loss = 0.5 * error**2\n",
    "        linear_loss = delta * (np.abs(error) - 0.5 * delta)\n",
    "        return np.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    huber = huber_loss(y_true, y_pred, delta=1.0)\n",
    "    huber\n",
    "    ```\n",
    "    \n",
    "    ```python\n",
    "    # 로그 코사인 유사도 (Log-Cosh Loss)\n",
    "    def log_cosh_loss(y_true, y_pred):\n",
    "        return np.mean(np.log(np.cosh(y_pred - y_true)))\n",
    "    \n",
    "    log_cosh = log_cosh_loss(y_true, y_pred)\n",
    "    log_cosh\n",
    "    ```\n",
    "    \n",
    "- 분류 문제\n",
    "    \n",
    "    ```python\n",
    "    from sklearn.metrics import log_loss, hinge_loss\n",
    "    import numpy as np\n",
    "    \n",
    "    # 예시 데이터: 실제 레이블과 예측 확률\n",
    "    y_true = np.array([1, 0, 1, 1, 0])\n",
    "    y_pred_probs = np.array([0.9, 0.1, 0.8, 0.65, 0.3]) # 이진 분류의 예측 확률\n",
    "    y_pred = np.array([1, 0, 1, 1, 0]) # 정답\n",
    "    \n",
    "    ```\n",
    "    \n",
    "    ```python\n",
    "    # 교차 엔트로피 손실\n",
    "    cross_entropy_loss = log_loss(y_true, y_pred_probs)\n",
    "    cross_entropy_loss\n",
    "    \n",
    "    ```\n",
    "    \n",
    "    ```python\n",
    "    # 힌지 손실\n",
    "    hinge_loss_value = hinge_loss(y_true, 2*y_pred-1)  # hinge_loss는 -1과 1의 레이블을 기대합니다.\n",
    "    hinge_loss_value\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
